# -*- coding: utf-8 -*-
"""FED Minutes - Dash

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GRRDsuDLyW8xjxFgAWvMXwzE9KGN2mkb

### Scraping FED scripts
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import pandas as pd
import re

# Existing import statements...
import logging

# Set up basic logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)

def scrape_93to95(url):
    logging.info(f"Scraping data from URL: {url}")
    response = requests.get(url)
    if response.status_code != 200:
        logging.error(f"Failed to fetch data from URL: {url} with status code: {response.status_code}")

    soup = BeautifulSoup(response.content, "html.parser")

    # Find all <p> tags within the relevant <div> or other elements unique to the 1993 version
    # You'll need to inspect the HTML of the 1993 version to identify the appropriate elements
    p_tags = soup.find_all('p')  # Adjust this based on the actual HTML structure


    # Define a regular expression pattern to match the date part
    date_pattern = r"/(\d{8})min\.htm"

    # Use re.search to find the date in the URL
    match = re.search(date_pattern, url)

    if match:
        date = match.group(1)
    else:
        print("Date not found in the URL.")
    scraped_text = []

    # Extract text from <p> tags, removing newlines and carriage returns
    url_text = ' '.join([p_tag.get_text().replace('\n', ' ').replace('\r', '') for p_tag in p_tags])
    return url_text, date
def scrape_1996to2007(url):
    logging.info(f"Scraping data from URL: {url}")
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    # Find the table with cellpadding="5"
    target_table = soup.find("table", {"cellpadding": "5"})
    url_parts = url.split('/')

    # Extract the date part (second-to-last element) from the URL
    date = url.split("/")[-1].split(".")[0]
    # Check if the table was found
    if target_table:
        # Find all <p> tags within the table
        p_tags = target_table.find_all("p")

        # Extract and join the text from <p> tags, removing newlines and carriage returns
        table_text = ' '.join([p.get_text().replace('\n', ' ').replace('\r', '') for p in p_tags])

        return table_text,date

    else:
        return None,date
def scrape_2007to2011(url):
    logging.info(f"Scraping data from URL: {url}")
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    # Find all <p> tags on the page
    p_tags = soup.find_all('p')
    date = url[-12:-4]
    url_text = ' '.join([p_tag.get_text().replace('\n', ' ').replace('\r', '') for p_tag in p_tags])

    return url_text,date
def scrape_2012to2017(url):
    logging.info(f"Scraping data from URL: {url}")
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    date = url[-12:-4]
    # Find all elements with class "col-xs-12 col-sm-8 col-md-9"
    target_elements = soup.find_all(class_="col-xs-12 col-sm-8 col-md-9")

    # Check if any elements were found
    if target_elements:
        # Find all <p> tags within the target elements
        p_tags = [element.find_all("p") for element in target_elements]

        # Extract and join the text from <p> tags, removing newlines and carriage returns
        all_text = ''
        for p_tag_group in p_tags:
            for p_tag in p_tag_group:
                all_text += p_tag.get_text().replace('\n', ' ').replace('\r', '') + ' '
        return all_text,date

    else:
        return None,date

def find_minutes_urls1(main_url):
    logging.info(f"Fetching main page: {main_url}")
    # Send an HTTP GET request to the main page
    response = requests.get(main_url)
    soup = BeautifulSoup(response.content, "html.parser")

    # Find and collect the links with names "1993", "1994", and "1995"
    year_links = soup.find_all("a", text=["1995", "1994", "1993"])

    minutes_urls = []

    # Get the base URL to construct absolute URLs
    base_url = main_url

    # Loop through the year links and visit each year's page
    for year_link in year_links:
        # Get the relative URL of the year's page
        year_url_relative = year_link["href"]

        # Construct an absolute URL for the year's page using urljoin
        year_url_absolute = urljoin(base_url, year_url_relative)

        # Send an HTTP GET request to the year's page
        year_response = requests.get(year_url_absolute)
        year_soup = BeautifulSoup(year_response.content, "html.parser")

        # Find and collect the URLs with the name "Minutes"
        minutes_links = year_soup.find_all("a", text="Minutes")

        # Extract the URLs from the "Minutes" links and append them to the list
        minutes_urls.extend([
            urljoin(year_url_absolute, minutes_link["href"])
            for minutes_link in minutes_links
            if minutes_link["href"].startswith("/fomc/MINUTES/") and not minutes_link["href"].endswith("#phone")
        ])
        minutes_urls.reverse()
    return minutes_urls
def find_minutes_urls2(main_url):
    logging.info(f"Fetching main page: {main_url}")
    # Send an HTTP GET request to the main page
    response = requests.get(main_url)
    soup = BeautifulSoup(response.content, "html.parser")

    # Find the link for "1996" to start the navigation
    start_year_link = soup.find("a", text="1996")

    if start_year_link is None:
        return []  # Return an empty list if the "1996" link is not found

    minutes_urls = []

    # Get the base URL to construct absolute URLs
    base_url = main_url

    # Loop through the years from 1996 to 2006
    for year in range(1996, 2008):
        # Construct the link text for the year
        year_text = str(year)

        # Find the link for the current year
        year_link = soup.find("a", text=year_text)

        # If the link is found, click on it and navigate to the year's page
        if year_link:
            year_url_relative = year_link["href"]
            year_url_absolute = urljoin(base_url, year_url_relative)

            # Send an HTTP GET request to the year's page
            year_response = requests.get(year_url_absolute)
            year_soup = BeautifulSoup(year_response.content, "html.parser")

            # Find and collect the URLs with the name "Minutes"
            minutes_links = year_soup.find_all("a", text="Minutes")

            # Extract and filter the URLs from the "Minutes" links
            minutes_urls.extend([
                urljoin(year_url_absolute, minutes_link["href"])
                for minutes_link in minutes_links
                if minutes_link["href"].startswith("/fomc/minutes/") and minutes_link["href"].endswith(".htm")
            ])

    # Filter out URLs that don't start with the desired pattern
    minutes_urls = [
        url
        for url in minutes_urls
        if url.startswith("https://www.federalreserve.gov/fomc/minutes/")
    ]
    return minutes_urls
def find_minutes_urls3(main_url):
    logging.info(f"Fetching main page: {main_url}")
    # Send an HTTP GET request to the main page
    response = requests.get(main_url)
    soup = BeautifulSoup(response.content, "html.parser")

    # Find and collect the links for years 2008 to 2011
    year_links = soup.find_all("a", text=["2008", "2009", "2010", "2011"])

    minutes_urls = []

    # Get the base URL to construct absolute URLs
    base_url = main_url

    # Loop through the year links and visit each year's page
    for year_link in year_links:
        # Get the URL of the year's page
        year_url_relative = year_link["href"]
        year_url_absolute = urljoin(base_url, year_url_relative)

        # Send an HTTP GET request to the year's page
        year_response = requests.get(year_url_absolute)
        year_soup = BeautifulSoup(year_response.content, "html.parser")

        # Find and collect the URLs with "/monetarypolicy/fomcminutes" and ending with ".htm"
        all_links = year_soup.find_all("a")

        for link in all_links:
            href = link.get("href")
            if href and "/monetarypolicy/fomcminutes" in href and href.endswith(".htm"):
                full_url = urljoin(year_url_absolute, href)
                minutes_urls.append(full_url)
    minutes_urls.append("https://www.federalreserve.gov/monetarypolicy/fomcminutes20071031.htm")
    minutes_urls.append("https://www.federalreserve.gov/monetarypolicy/fomcminutes20071211.htm")
    minutes_urls.reverse()
    return minutes_urls
def find_minutes_urls4(main_url):
    logging.info(f"Fetching main page: {main_url}")
    # Send an HTTP GET request to the main page
    response = requests.get(main_url)
    soup = BeautifulSoup(response.content, "html.parser")

    # Find and collect the links for years 2012 to 2017
    year_links = soup.find_all("a", text=["2012", "2013", "2014", "2015", "2016", "2017"])

    minutes_urls = []

    # Get the base URL to construct absolute URLs
    base_url = main_url

    # Loop through the year links and visit each year's page
    for year_link in year_links:
        # Get the URL of the year's page
        year_url_relative = year_link["href"]
        year_url_absolute = urljoin(base_url, year_url_relative)

        # Send an HTTP GET request to the year's page
        year_response = requests.get(year_url_absolute)
        year_soup = BeautifulSoup(year_response.content, "html.parser")

        # Find and collect the URLs with "/monetarypolicy/fomcminutes" and ending with ".htm"
        all_links = year_soup.find_all("a")

        for link in all_links:
            href = link.get("href")
            if href and "/monetarypolicy/fomcminutes" in href and href.endswith(".htm"):
                full_url = urljoin(year_url_absolute, href)
                minutes_urls.append(full_url)

    # Reverse the order of collected URLs
    minutes_urls.reverse()

    return minutes_urls
# Example usage:
# Replace with the actual URL of the main page

def find_minutes_urls_after_2017(main_url):
    logging.info(f"Fetching main page: {main_url}")
    response = requests.get(main_url)
    soup = BeautifulSoup(response.content, "html.parser")

    minutes_urls = []

    # Find all links that include "/monetarypolicy/fomcminutes" and end with ".htm"
    all_links = soup.find_all("a", href=True)
    for link in all_links:
        href = link.get("href")
        if href and "/monetarypolicy/fomcminutes" in href and href.endswith(".htm"):
            full_url = urljoin(main_url, href)
            minutes_urls.append(full_url)

    return minutes_urls
# Example usage:
scraped_data = []

# Scrape data for 1993 to 1995
main_page_url_1993_to_1995 = "https://www.federalreserve.gov/monetarypolicy/fomc_historical_year.htm"
minutes_urls_1993_to_1995 = find_minutes_urls1(main_page_url_1993_to_1995)
minutes_urls_1993_to_1995.reverse()
for url in minutes_urls_1993_to_1995:
    date, text = scrape_93to95(url)
    scraped_data.append([date, text])

# Scrape data for 1996 to 2007
main_page_url_1996_to_2007 = "https://www.federalreserve.gov/monetarypolicy/fomc_historical_year.htm"
minutes_urls_1996_to_2007 = find_minutes_urls2(main_page_url_1996_to_2007)
for url in minutes_urls_1996_to_2007:
    text, date = scrape_1996to2007(url)
    if date:
        scraped_data.append([date, text])

# Scrape data for 2007 to 2011
main_page_url_2007_to_2011 = "https://www.federalreserve.gov/monetarypolicy/fomc_historical_year.htm"
minutes_urls_2007_to_2011 = find_minutes_urls3(main_page_url_2007_to_2011)
for url in minutes_urls_2007_to_2011:
    text, date = scrape_2007to2011(url)
    if date:
        scraped_data.append([date, text])

# Scrape data for 2012 to 2017
main_page_url_2012_to_2017 = "https://www.federalreserve.gov/monetarypolicy/fomc_historical_year.htm"
minutes_urls_2012_to_2017 = find_minutes_urls4(main_page_url_2012_to_2017)
for url in minutes_urls_2012_to_2017:
    text, date = scrape_2012to2017(url)
    if date:
        scraped_data.append([date, text])

# Scrape data for years after 2017
main_page_url_after_2017 = "https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm"
minutes_urls_after_2017 = find_minutes_urls_after_2017(main_page_url_after_2017)
for url in minutes_urls_after_2017:
    text, date = scrape_2012to2017(url)
    if date:
        scraped_data.append([date, text])

# Create a Pandas DataFrame with date and text columns
df = pd.DataFrame(scraped_data, columns=["Date", "Text"])
# Save the DataFrame to a CSV file
df.to_csv("./data/scraped_data_all_years_true.csv", index=False)
#df = pd.read_csv("scraped_data_all_years_true-2.csv")